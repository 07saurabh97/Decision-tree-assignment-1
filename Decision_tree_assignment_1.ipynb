{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "iSXVwhHUo18_",
        "outputId": "7ab6a703-05a9-4a6f-c430-a644de33a7ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' A Decision Tree Classifier is a supervised machine learning algorithm used for classification tasks. It predicts the label of a data point by traversing\\nthrough a tree structure, where each internal node represents a decision based on an input feature, each branch represents an outcome of that decision,\\nand each leaf node represents a class label.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Q1. Describe the decision tree classifier algorithm and how it works to make predictions.\n",
        "''' A Decision Tree Classifier is a supervised machine learning algorithm used for classification tasks. It predicts the label of a data point by traversing\n",
        "through a tree structure, where each internal node represents a decision based on an input feature, each branch represents an outcome of that decision,\n",
        "and each leaf node represents a class label.\n",
        "\n",
        "How it works -\n",
        "1. Tree Structure:\n",
        "-The tree starts at a root node and grows into branches and leaves.\n",
        "-Each internal node splits the data based on a specific feature and a condition.\n",
        "-The process continues recursively until a stopping condition is met\n",
        "(e.g., maximum depth, minimum samples per leaf, or no further splits improve the model).\n",
        "\n",
        "2. Training Phase:\n",
        "Feature Selection: The algorithm selects the feature and the condition (threshold) that best splits the data into homogeneous groups (where most data points\n",
        "in a group belong to the same class).\n",
        "  Splitting Criteria:\n",
        "  Gini Impurity: measures the impurity of a node. A node is called pure if all its data points belong the same class\n",
        "  Entrropy: Measures the disorder or randomness in the data at a node.\n",
        "  Information Gain: Used to evaluate how much a split improves the purity of the resulting subsets:\n",
        "3. Prediction Phase: For a new data point, the algorithm starts at the root node and follows the branches corresponding to the conditions satisfied by the input\n",
        " features.\n",
        "   It continues until it reaches a leaf node, which holds the predicted class label.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.\n",
        "'''A decision tree classification algorithm works by recursively splitting a dataset into smaller subsets based on the values of features, aiming to maximize\n",
        "the \"purity\" of each subset (meaning most data points within a subset belong to the same class) at each split, using a metric like \"information gain\" calculated\n",
        " through entropy to guide the decision-making process\n",
        "\n",
        "Key steps in the mathematical intuition of decision tree classification:\n",
        "1. Entropy Calculation: Entropy measures the level of uncertainty or impurity in a dataset, where high entropy indicates a mix of classes and low entropy\n",
        " indicates a mostly homogenous class distribution.\n",
        "\n",
        "2. Information Gain Calculation:\n",
        "Concept: Information gain is the decrease in entropy achieved by splitting a dataset based on a feature.\n",
        "  -Calculate the entropy of the parent node (original dataset).\n",
        "  -For each possible split on a feature, calculate the weighted average entropy of the child nodes.\n",
        "  -Information Gain = Entropy(parent) - (weighted average entropy of child nodes).\n",
        "\n",
        "3. Spliting the best dataset\n",
        "\n",
        "4. Decision tree structure:\n",
        "  -Nodes: Each internal node in the tree represents a decision based on a feature value.\n",
        "  -Branches: Each branch represents a possible outcome of the decision at the node.\n",
        "  -Leaves: Leaf nodes represent the final predicted class label for a data point reaching that point in the tree.\n",
        "\n",
        "Important Considerations:\n",
        "Overfitting:\n",
        "Decision trees can easily overfit to training data, meaning they perform poorly on new data due to complex tree structures. Techniques like pruning\n",
        "(removing branches) are used to mitigate this issue.\n",
        "'''"
      ],
      "metadata": {
        "id": "DOxpXtWnpiC9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "d9839e8a-f1ce-4b59-b7e2-8c5943ffba21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A decision tree classification algorithm works by recursively splitting a dataset into smaller subsets based on the values of features, aiming to maximize\\nthe \"purity\" of each subset (meaning most data points within a subset belong to the same class) at each split, using a metric like \"information gain\" calculated\\n through entropy to guide the decision-making process\\n\\nKey steps in the mathematical intuition of decision tree classification:\\n1. Entropy Calculation: Entropy measures the level of uncertainty or impurity in a dataset, where high entropy indicates a mix of classes and low entropy\\n indicates a mostly homogenous class distribution.  \\n\\n2. Information Gain Calculation:\\nConcept: Information gain is the decrease in entropy achieved by splitting a dataset based on a feature.  \\n  -Calculate the entropy of the parent node (original dataset).\\n  -For each possible split on a feature, calculate the weighted average entropy of the child nodes.\\n  -Information Gain = Entropy(parent) - (weighted average entropy of child nodes).\\n\\n3. Spliting the best dataset\\n\\n4. Decision tree structure:\\n  -Nodes: Each internal node in the tree represents a decision based on a feature value. \\n  -Branches: Each branch represents a possible outcome of the decision at the node. \\n  -Leaves: Leaf nodes represent the final predicted class label for a data point reaching that point in the tree. \\n\\nImportant Considerations:\\nOverfitting:\\nDecision trees can easily overfit to training data, meaning they perform poorly on new data due to complex tree structures. Techniques like pruning \\n(removing branches) are used to mitigate this issue.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3. Explain how a decision tree classifier can be used to solve a binary classification problem."
      ],
      "metadata": {
        "id": "EG-TVKsZUT4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' A decision tree classifier solves a binary classification problem by recursively splitting the input data into subsets based on feature values to maximize\n",
        "the separation between the two target classes (e.g., Class 0 and Class 1).\n",
        "A decision tree classifier solves a binary classification problem by creating a hierarchical structure of \"if-then\" rules, where each node represents a question\n",
        " based on a feature of the data, and the branches represent possible answers, ultimately leading to a leaf node that provides the final classification\n",
        " (either \"yes\" or \"no\" in a binary problem) based on the path taken through the tree, effectively dividing the data into smaller subsets until a clear\n",
        " prediction can be made for each data point.\n",
        "\n",
        "Key steps in using a decision tree for binary classification:\n",
        "1. Data Splitting:\n",
        "The algorithm begins by examining the entire dataset at the root node and selects the \"best\" feature to split the data based on a metric like Gini impurity or\n",
        "information gain, effectively dividing the data into two subsets based on the feature value.\n",
        "2. Recursive Splitting:\n",
        "Each of these subsets is then further split using the same process, selecting the most informative feature at each node to create child nodes until a stopping\n",
        "criterion is met, such as reaching a maximum depth or having a minimum number of samples per leaf.\n",
        "3. Leaf Node Assignment:\n",
        "At the end of each branch (leaf node), the majority class label from the data points that reached that node is assigned as the predicted class.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "Bu6FJ8A-UabB",
        "outputId": "d678663a-fe4d-455f-e32c-ab5bc4e7dbbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' A decision tree classifier solves a binary classification problem by recursively splitting the input data into subsets based on feature values to maximize \\nthe separation between the two target classes (e.g., Class 0 and Class 1).\\nA decision tree classifier solves a binary classification problem by creating a hierarchical structure of \"if-then\" rules, where each node represents a question\\n based on a feature of the data, and the branches represent possible answers, ultimately leading to a leaf node that provides the final classification \\n (either \"yes\" or \"no\" in a binary problem) based on the path taken through the tree, effectively dividing the data into smaller subsets until a clear \\n prediction can be made for each data point. \\n\\nKey steps in using a decision tree for binary classification:\\n1. Data Splitting:\\nThe algorithm begins by examining the entire dataset at the root node and selects the \"best\" feature to split the data based on a metric like Gini impurity or \\ninformation gain, effectively dividing the data into two subsets based on the feature value. \\n2. Recursive Splitting:\\nEach of these subsets is then further split using the same process, selecting the most informative feature at each node to create child nodes until a stopping \\ncriterion is met, such as reaching a maximum depth or having a minimum number of samples per leaf. \\n3. Leaf Node Assignment:\\nAt the end of each branch (leaf node), the majority class label from the data points that reached that node is assigned as the predicted class. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iHsh3YGYUdQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make predictions."
      ],
      "metadata": {
        "id": "N0AttDqNVzUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The process of ‚Äúmarking the cut‚Äù involves selecting a specific threshold or value for a feature that optimally divides the data into distinct branches.\n",
        "This threshold is chosen in a way that minimizes the impurity within each resulting branch.\n",
        "\n",
        "Geometric Representation\n",
        " Axis-Aligned Splits:\n",
        "  The splits are perpendicular to the axes of the features.\n",
        "  For a dataset with two features (ùë•1 and x1):\n",
        "    A split like x1 < 5 dicidees the features space into two vertical regions.\n",
        "    A split like x2 < 3 divides the features space into two horizontal regions.\n",
        "  As more splits are made, the space is divided into finer regions.\n",
        "\n",
        "Leaf Nodes:\n",
        "  Each leaf node corresponds to a subregion of the feature space.\n",
        "  The class label assigned to a region is determined by the majority class of the data points within that region.\n",
        "\n",
        "‚Äã\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "ay0-MBIMV2dO",
        "outputId": "6ffa25f8-d1d1-431c-8907-9e8e868c0bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\nThe process of ‚Äúmarking the cut‚Äù involves selecting a specific threshold or value for a feature that optimally divides the data into distinct branches. \\nThis threshold is chosen in a way that minimizes the impurity within each resulting branch.\\n\\nGeometric Representation\\n Axis-Aligned Splits:\\n  The splits are perpendicular to the axes of the features.\\n  For a dataset with two features (ùë•1 and x1):\\n    A split like x1 < 5 dicidees the features space into two vertical regions.\\n    A split like x2 < 3 divides the features space into two horizontal regions.\\n  As more splits are made, the space is divided into finer regions.\\n\\nLeaf Nodes:\\n  Each leaf node corresponds to a subregion of the feature space.\\n  The class label assigned to a region is determined by the majority class of the data points within that region.\\n\\n\\u200b\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a classification model."
      ],
      "metadata": {
        "id": "si9MKhykXvI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The confusion matrix is a table used to evaluate the performance of a classification model by comparing the predicted labels with the true labels.\n",
        "It provides detailed insights into how well the model is performing for each class by counting the number of correct and incorrect predictions.\n",
        "\n",
        "Structure of a Confusion Matrix\n",
        "For a binary classification problem with two classes (e.g., Positive and Negative), the confusion matrix is a 2x2 table:\n",
        "\n",
        "                 Predicted Positive\t  Predicted Negative\n",
        "Actual Positive\tTrue Positive (TP)\tFalse Negative (FN)\n",
        "Actual Negative\tFalse Positive (FP)\tTrue Negative (TN)\n",
        "\n",
        "- True Positive (TP): Correctly predicted positive instances.\n",
        "- True Negative (TN): Correctly predicted negative instances.\n",
        "- False Positive (FP): Instances incorrectly predicted as positive (Type I error).\n",
        "- False Negative (FN): Instances incorrectly predicted as negative (Type II error).\n",
        "\n",
        "Key Metrics Derived from the Confusion Matrix\n",
        "1. Accuracy(A): A=TP+TN/TP+TN+FP+FN\n",
        "2. Precision(P): P=TP/TP+FP\n",
        "3. Recall(R): R=TP/TP+FN\n",
        "4. F1 Score: F1=2*P*R/(P+R)\n",
        "5. False positive rate(FPR): FPR=FP/FP+TN\n",
        "6. False negative rate(FNR): FNR=FN/FN+TP\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "be-RmLocXz0i",
        "outputId": "8358a270-ad04-4980-ca95-a8f928075406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe confusion matrix is a table used to evaluate the performance of a classification model by comparing the predicted labels with the true labels. \\nIt provides detailed insights into how well the model is performing for each class by counting the number of correct and incorrect predictions.\\n\\nStructure of a Confusion Matrix\\nFor a binary classification problem with two classes (e.g., Positive and Negative), the confusion matrix is a 2x2 table:\\n\\n                 Predicted Positive\\t  Predicted Negative\\nActual Positive\\tTrue Positive (TP)\\tFalse Negative (FN)\\nActual Negative\\tFalse Positive (FP)\\tTrue Negative (TN)\\n\\n- True Positive (TP): Correctly predicted positive instances.\\n- True Negative (TN): Correctly predicted negative instances.\\n- False Positive (FP): Instances incorrectly predicted as positive (Type I error).\\n- False Negative (FN): Instances incorrectly predicted as negative (Type II error).\\n\\nKey Metrics Derived from the Confusion Matrix\\n1. Accuracy(A): A=TP+TN/TP+TN+FP+FN\\n2. Precision(P): P=TP/TP+FP\\n3. Recall(R): R=TP/TP+FN\\n4. F1 Score: F1=2*P*R/(P+R)\\n5. False positive rate(FPR): FPR=FP/FP+TN\\n6. False negative rate(FNR): FNR=FN/FN+TP\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be calculated from it."
      ],
      "metadata": {
        "id": "jOuh3UHTY75y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score,f1_score"
      ],
      "metadata": {
        "id": "yVT-3HwRZDZv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual=np.array(['Dog','Dog','Dog','Not Dog','Dog','Not Dog','Dog','Dog','Not Dog','Not Dog'])\n",
        "predicted=np.array(['Dog','Not Dog','Dog','Not Dog','Dog','Dog','Dog','Dog','Dog','Dog'])"
      ],
      "metadata": {
        "id": "GEC-c6IgWhVo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm=confusion_matrix(actual,predicted, labels=['Dog','Not Dog'])"
      ],
      "metadata": {
        "id": "dfmA39LrXrsX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TN,FP,FN,TP =cm.ravel()"
      ],
      "metadata": {
        "id": "zuiNfiQdXMX8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision=precision_score(actual, predicted, pos_label='Dog')\n",
        "recall=recall_score(actual, predicted, pos_label='Dog')\n",
        "f1=f1_score(actual, predicted, pos_label='Dog')"
      ],
      "metadata": {
        "id": "0N2KhRCzXgnP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"confusion matrix: \\n{cm}\")\n",
        "print(f\"precision: {precision:}\")\n",
        "print(f\"recall: {recall}\")\n",
        "print(f\"f1: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wYqaS9LZTET",
        "outputId": "1e321dfc-e613-4abd-b51c-8eb5a1bd5e0e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion matrix: \n",
            "[[5 1]\n",
            " [3 1]]\n",
            "precision: 0.625\n",
            "recall: 0.8333333333333334\n",
            "f1: 0.7142857142857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and explain how this can be done."
      ],
      "metadata": {
        "id": "Hid1pBsVah4q"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "choosing an appropriate evaluation metrics for a classification prroblem is curcial because it directly impacts the assessment of the mmodel's performance\n",
        "Why choosing the right metric?\n",
        "1. Model Selection\n",
        "2. Hyperperameter tuning\n",
        "3. Model comparision\n",
        "\n",
        "commom=n evaluation metrics for classification problems\n",
        "1. Accuracy\n",
        "2. Precision\n",
        "3. Recall\n",
        "4. F1 Scoree\n",
        "5. Area Under the Precision-Recall Curve (AUC-PR)\n",
        "6. Confusion Matrix\n",
        "\n",
        "How to choose right metrics\n",
        "1. problem Type\n",
        "2. Class distribution\n",
        "3. Model Complexity\n",
        "4. Business objectives\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "WbA5h7HJbLtu",
        "outputId": "79d99eea-7b4e-4b28-a38f-a8997b752fd3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" \\nchoosing an appropriate evaluation metrics for a classification prroblem is curcial because it directly impacts the assessment of the mmodel's performance\\nWhy choosing the right metric?\\n1. Model Selection\\n2. Hyperperameter tuning\\n3. Model comparision\\n\\ncommom=n evaluation metrics for classification problems\\n1. Accuracy\\n2. Precision\\n3. Recall\\n4. F1 Scoree\\n5. Area Under the Precision-Recall Curve (AUC-PR)\\n6. Confusion Matrix\\n\\nHow to choose right metrics\\n1. problem Type\\n2. Class distribution\\n3. Model Complexity\\n4. Business objectives\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8. Provide an example of a classification problem where precision is the most important metric, and explain why."
      ],
      "metadata": {
        "id": "ByOK7Lv0e-hw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''A classic example of a classification problem where precision is the most important metric is spam email detection; in this scenario, it's crucial to minimize false positives (flagging non-spam emails as spam), so ensuring that when an email is classified as spam, it is highly likely to be actual spam is prioritized over catching every single spam email, making precision the key metric to focus on.\n",
        "Why precision is key in this case:\n",
        "  User experience:\n",
        "False positives can irritate users by flagging important emails as spam, leading to potential loss of crucial information.\n",
        "  Trust in the system:\n",
        "High precision builds user confidence in the spam filter, as they are less likely to miss important emails due to false positives.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "DZWCv2u7fWxn",
        "outputId": "ac94d261-d7ed-4d28-bc62-cf8df595d3d2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A classic example of a classification problem where precision is the most important metric is spam email detection; in this scenario, it's crucial to minimize false positives (flagging non-spam emails as spam), so ensuring that when an email is classified as spam, it is highly likely to be actual spam is prioritized over catching every single spam email, making precision the key metric to focus on. \\nWhy precision is key in this case:\\n  User experience:\\nFalse positives can irritate users by flagging important emails as spam, leading to potential loss of crucial information.\\n  Trust in the system:\\nHigh precision builds user confidence in the spam filter, as they are less likely to miss important emails due to false positives. \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9. Provide an example of a classification problem where recall is the most important metric, and explain why."
      ],
      "metadata": {
        "id": "Mxqh7gZdfbaO"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''example of a classification problem where recall is the most important metric is medical diagnosis, specifically detecting a serious disease like cancer;\n",
        "in this scenario, it's crucial to identify almost all actual cases of the disease (high recall) even if it means potentially raising a few false alarms\n",
        " (lower precision), because missing a case of cancer could have severe consequences for the patient.\n",
        "\n",
        "Why recall is prioritized here:\n",
        "  Cost of false negatives is high:\n",
        "In medical diagnosis, a false negative (missing a cancer case) can lead to delayed treatment and potentially worse health outcomes, making it much more\n",
        "critical than a false positive (misdiagnosing someone as having cancer) which can be investigated further with additional tests.\n",
        "  Early detection is key:\n",
        "For diseases like cancer, early detection is crucial for successful treatment, so a model needs to identify as many potential cases as possible, even if\n",
        "it means some uncertainty\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "P84duH9yfrke",
        "outputId": "9f789746-a3c6-4cac-d478-1217336aee17"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"example of a classification problem where recall is the most important metric is medical diagnosis, specifically detecting a serious disease like cancer; \\nin this scenario, it's crucial to identify almost all actual cases of the disease (high recall) even if it means potentially raising a few false alarms\\n (lower precision), because missing a case of cancer could have severe consequences for the patient. \\n \\nWhy recall is prioritized here:\\n  Cost of false negatives is high:\\nIn medical diagnosis, a false negative (missing a cancer case) can lead to delayed treatment and potentially worse health outcomes, making it much more\\ncritical than a false positive (misdiagnosing someone as having cancer) which can be investigated further with additional tests.\\n  Early detection is key:\\nFor diseases like cancer, early detection is crucial for successful treatment, so a model needs to identify as many potential cases as possible, even if\\nit means some uncertainty\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LTrTjUvffs-N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}